import zipfile
import gzip
import io
import os
from datetime import datetime, timedelta
import asyncio
import aiohttp
import logging
import gc

# –õ–æ–≥–≥–µ—Ä —Å–æ–∑–¥–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –≤—Å–µ—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
logger = logging.getLogger(__name__)

AMPLITUDE_CLIENT_ID = os.getenv("AMPLITUDE_CLIENT_ID")
AMPLITUDE_SECRET_ID = os.getenv("AMPLITUDE_SECRET_ID")

async def download_day(day: str):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∑–∞ –¥–µ–Ω—å in-memory."""
    start = f"{day}T00"
    end = f"{day}T23"
    url = f"https://amplitude.com/api/2/export?start={start}&end={end}"

    auth = aiohttp.BasicAuth(AMPLITUDE_CLIENT_ID, AMPLITUDE_SECRET_ID)

    logger.info(f"üîÑ –ù–∞—á–∞–ª–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–Ω—è {day}")
    logger.debug(f"üì§ –ó–∞–ø—Ä–æ—Å –∫ Amplitude –¥–ª—è –¥–Ω—è {day}: {url}")
    
    async with aiohttp.ClientSession() as session:
        logger.debug(f"üöÄ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Amplitude –¥–ª—è –¥–Ω—è {day}")
        async with session.get(url, auth=auth) as response:
            logger.debug(f"üì• –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Amplitude –¥–ª—è –¥–Ω—è {day}: —Å—Ç–∞—Ç—É—Å {response.status}")
            
            if response.status == 404:
                error_text = await response.text()
                if "Raw data files were not found." in error_text:
                    logger.info(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–Ω—è {day}")
                    return []
                else:
                    raise ValueError(f"–û—à–∏–±–∫–∞ Amplitude: {response.status} - {error_text}")
            if response.status != 200:
                raise ValueError(f"–û—à–∏–±–∫–∞ Amplitude: {response.status} - {await response.text()}")
                
            zip_content = await response.read()
    
    logger.info(f"‚úÖ –î–µ–Ω—å {day} —Å–∫–∞—á–∞–Ω —É—Å–ø–µ—à–Ω–æ, —Ä–∞–∑–º–µ—Ä: {len(zip_content) / 1024 / 1024:.2f} MB")

    logger.debug(f"üóúÔ∏è –ù–∞—á–∞–ª–æ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ ZIP –¥–ª—è –¥–Ω—è {day}")
    with zipfile.ZipFile(io.BytesIO(zip_content)) as zip_ref:
        gz_files = [name for name in zip_ref.namelist() if name.endswith(".gz")]
        logger.debug(f"üìÅ –ù–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –≤ ZIP: {gz_files}")
        
        if not gz_files:
            raise ValueError("–ù–µ—Ç .gz —Ñ–∞–π–ª–æ–≤ –≤ zip")

        all_lines = []
        for gz_name in gz_files:
            logger.debug(f"üîì –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ —Ñ–∞–π–ª–∞: {gz_name}")
            with zip_ref.open(gz_name) as gz_file:
                gz_content = gz_file.read()
                json_content = gzip.decompress(gz_content)
                lines = json_content.decode("utf-8").splitlines()
                all_lines.extend(lines)
                logger.debug(f"‚úÖ –†–∞—Å–ø–∞–∫–æ–≤–∞–Ω {gz_name}: {len(lines)} —Å—Ç—Ä–æ–∫")

    logger.info(f"‚úÖ –î–µ–Ω—å {day} —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω, —Å–æ–±—ã—Ç–∏–π: {len(all_lines)}")
    return all_lines


async def process_week(year: int, week: int, week_days: list, s3_client, semaphore_value: int = 7):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –Ω–µ–¥–µ–ª—é —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç—å—é."""
    logger.info(f"üöÄ –ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò –ù–ï–î–ï–õ–ò {year}_week_{week} (–¥–Ω–µ–π: {len(week_days)})")
    semaphore = asyncio.Semaphore(semaphore_value)

    week_data = {}
    no_data_days = []

    async def process_day(day):
        async with semaphore:
            logger.info(f"üì• –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–Ω—è {day} –≤ –Ω–µ–¥–µ–ª–µ {year}_{week}")
            try:
                lines = await download_day(day)
                if lines:
                    week_data[day] = lines
                    logger.info(f"‚úÖ –î–µ–Ω—å {day} –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ, —Å–æ–±—ã—Ç–∏–π: {len(lines)}")
                    
                    del lines
                    gc.collect()
                else:
                    no_data_days.append(day)
                    logger.info(f"‚ûñ –î–µ–Ω—å {day} –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è –¥–Ω—è {day} –≤ –Ω–µ–¥–µ–ª–µ {year}_{week}: {str(e)}")
                raise

    logger.info(f"‚è≥ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ {len(week_days)} –¥–Ω–µ–π –Ω–µ–¥–µ–ª–∏ {year}_{week}")
    await asyncio.gather(*(process_day(day) for day in week_days))
    logger.info(f"‚úÖ –í—Å–µ –¥–Ω–∏ –Ω–µ–¥–µ–ª–∏ {year}_{week} —Å–∫–∞—á–∞–Ω—ã")

    logger.info(f"üß© –°–±–æ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–µ–ª–∏ {year}_{week}")
    ndjson_io = io.StringIO()
    total_lines = 0
    
    for day in sorted(week_days):
        if day in week_data:
            lines = week_data[day]
            for line in lines:
                ndjson_io.write(line + "\n")
            total_lines += len(lines)
            del week_data[day]
            gc.collect()
    
    ndjson_content = ndjson_io.getvalue().encode("utf-8")
    ndjson_io.close()
    
    del week_data
    gc.collect()

    logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –Ω–µ–¥–µ–ª–∏ {year}_{week} —Å–æ–±—Ä–∞–Ω—ã, –≤—Å–µ–≥–æ —Å–æ–±—ã—Ç–∏–π: {total_lines}")

    logger.info(f"üóúÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ ZIP-–∞—Ä—Ö–∏–≤–∞ –¥–ª—è –Ω–µ–¥–µ–ª–∏ {year}_{week}")
    zip_io = io.BytesIO()
    with zipfile.ZipFile(zip_io, "w", zipfile.ZIP_DEFLATED) as new_zip:
        new_zip.writestr(f"{year}_week_{week}.ndjson", ndjson_content)
    zip_io.seek(0)

    zip_size = len(zip_io.getvalue()) / 1024 / 1024
    logger.info(f"‚úÖ ZIP —Å–æ–∑–¥–∞–Ω, —Ä–∞–∑–º–µ—Ä: {zip_size:.2f} MB")

    s3_object_name = f"amplitude/{year}_week_{week}.zip"
    logger.info(f"‚òÅÔ∏è –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ S3: {s3_object_name}")
    logger.debug(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ S3: {s3_object_name}, —Ä–∞–∑–º–µ—Ä: {zip_size:.2f} MB")
    
    s3_client.client.put_object(
        Bucket=s3_client.bucket, Key=s3_object_name, Body=zip_io.getvalue()
    )
    
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ S3 –¥–ª—è –Ω–µ–¥–µ–ª–∏ {year}_{week}")
    logger.debug(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –≤ S3 –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {s3_object_name}")

    del ndjson_content, zip_io
    gc.collect()
    
    logger.info(f"üéâ –ó–ê–í–ï–†–®–ï–ù–ê –ù–ï–î–ï–õ–Ø {year}_week_{week}. –î–Ω–µ–π –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö: {no_data_days}")
    return f"{year}_week_{week}", no_data_days

def parse_dates(start_day: str, end_day: str):
    """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—ã –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–Ω–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYYMMDD."""
    start_dt = datetime.strptime(start_day[:8], "%Y%m%d")
    end_dt = datetime.strptime(end_day[:8], "%Y%m%d")
    days = []
    current = start_dt
    while current <= end_dt:
        days.append(current.strftime("%Y%m%d"))
        current += timedelta(days=1)
    logger.info(f"üìÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –ø–µ—Ä–∏–æ–¥: {len(days)} –¥–Ω–µ–π —Å {days[0]} –ø–æ {days[-1]}")
    return days