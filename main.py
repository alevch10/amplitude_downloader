import logger_config  # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–≤—ã–º –∏–º–ø–æ—Ä—Ç–æ–º!
import logging
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel
from s3_client import S3Client
from amplitude import process_week, parse_dates
import asyncio
import json
import gc
from datetime import datetime
from dotenv import load_dotenv
from collections import defaultdict
import psutil
import tracemalloc
import os

logger = logging.getLogger(__name__)

load_dotenv()


PROCESSED_FILE = "processed_history.json"
if not os.path.exists(PROCESSED_FILE):
    with open(PROCESSED_FILE, "w") as f:
        json.dump([], f)

app = FastAPI()

s3_client = S3Client()

current_processing = {"date": None, "week": None, "processed": []}
errors = []

tracemalloc.start()

def log_memory_usage(step_name=""):
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    memory_mb = memory_info.rss / 1024 / 1024
    logger.info(f"üíæ –ü–∞–º—è—Ç—å {step_name}: {memory_mb:.2f} MB")
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug(f"üß† –î–µ—Ç–∞–ª–∏ –ø–∞–º—è—Ç–∏ {step_name}: RSS={memory_mb:.2f}MB")

def clear_memory():
    gc.collect()
    log_memory_usage("–ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏")

class DateRange(BaseModel):
    start_day: str
    end_day: str

@app.get("/current_date")
async def get_current_date():
    return {
        "current_date": current_processing["date"],
        "current_week": current_processing["week"],
        "processed": current_processing["processed"],
    }

@app.get("/errors")
async def get_errors():
    return {"errors": errors}

@app.get("/memory")
async def get_memory_info():
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return {
        "memory_rss_mb": memory_info.rss / 1024 / 1024,
        "memory_vms_mb": memory_info.vms / 1024 / 1024,
    }

@app.post("/process")
async def process_dates(date_range: DateRange, background_tasks: BackgroundTasks):
    try:
        logger.info("üì® –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞—Ç")
        logger.debug(f"üîç –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞: start_day={date_range.start_day}, end_day={date_range.end_day}")
        
        days = parse_dates(date_range.start_day, date_range.end_day)
        background_tasks.add_task(run_processing, days)
        
        logger.info(f"üöÄ –ó–∞–ø—É—â–µ–Ω–∞ —Ñ–æ–Ω–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è {len(days)} –¥–Ω–µ–π")
        log_memory_usage("–ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        return {"status": "started", "days": days}
    except ValueError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç: {e}")
        raise HTTPException(status_code=400, detail=str(e))

async def run_processing(days: list):
    global current_processing, errors
    current_processing["processed"] = []
    errors = []

    logger.info(f"üé¨ –ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò –ü–ï–†–ò–û–î–ê: {days[0]} - {days[-1]} (–¥–Ω–µ–π: {len(days)})")
    logger.debug(f"üìã –î–Ω–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {days}")
    log_memory_usage("–≤ –Ω–∞—á–∞–ª–µ run_processing")

    weeks = defaultdict(list)
    for day in days:
        dt = datetime.strptime(day, "%Y%m%d")
        year, week, _ = dt.isocalendar()
        weeks[(year, week)].append(day)
    
    logger.info(f"üìà –ü–µ—Ä–∏–æ–¥ —Ä–∞–∑–¥–µ–ª—ë–Ω –Ω–∞ {len(weeks)} –Ω–µ–¥–µ–ª—å")
    logger.debug(f"üìä –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –Ω–µ–¥–µ–ª—è–º: {dict(weeks)}")

    processed_weeks = []
    week_count = 0
    total_weeks = len(weeks)
    
    for (year, week), week_days in sorted(weeks.items()):
        week_count += 1
        current_processing["week"] = f"{year}_week_{week}"
        
        logger.info(f"üî∑ [{week_count}/{total_weeks}] –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–¥–µ–ª–∏ {year}_week_{week} ({len(week_days)} –¥–Ω–µ–π)")
        logger.debug(f"üîç –î–Ω–∏ –Ω–µ–¥–µ–ª–∏ {year}_{week}: {week_days}")
        log_memory_usage(f"–ø–µ—Ä–µ–¥ –Ω–µ–¥–µ–ª–µ–π {year}_{week}")
        
        try:
            processed_id, no_data_days = await process_week(year, week, week_days, s3_client)
            processed_weeks.append(processed_id)
            current_processing["processed"].append(processed_id)

            entry = {
                "timestamp": datetime.now().isoformat(),
                "week": processed_id,
                "days": week_days,
                "no_data_days": no_data_days,
                "errors": False,
            }
            
            with open(PROCESSED_FILE, "r+") as f:
                history = json.load(f)
                history.append(entry)
                f.seek(0)
                json.dump(history, f, indent=4)
            
            logger.info(f"üíæ –ù–µ–¥–µ–ª—è {processed_id} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é")
            logger.debug(f"üìù –ó–∞–ø–∏—Å—å –≤ –∏—Å—Ç–æ—Ä–∏—é: {entry}")
            
            clear_memory()
            
            logger.info(f"‚úÖ [{week_count}/{total_weeks}] –ù–µ–¥–µ–ª—è {year}_week_{week} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è –Ω–µ–¥–µ–ª–∏ {year}_{week}: {str(e)}"
            errors.append(error_msg)
            logger.error(error_msg)
            
            entry = {
                "timestamp": datetime.now().isoformat(),
                "week": f"{year}_week_{week}",
                "days": week_days,
                "no_data_days": [],
                "errors": True,
            }
            
            with open(PROCESSED_FILE, "r+") as f:
                history = json.load(f)
                history.append(entry)
                f.seek(0)
                json.dump(history, f, indent=4)
            
            clear_memory()
            logger.error(f"‚ùå [{week_count}/{total_weeks}] –ù–µ–¥–µ–ª—è {year}_week_{week} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–æ–π")
            
        finally:
            current_processing["week"] = None

    logger.info(f"üéâ –í–°–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ùA. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–µ–¥–µ–ª—å: {len(processed_weeks)}, –û—à–∏–±–æ–∫: {len(errors)}")
    log_memory_usage("–≤ –∫–æ–Ω—Ü–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")

@app.get("/logs")
async def get_logs():
    if os.path.exists("app.log"):
        return FileResponse("app.log", filename="app.log", media_type="text/plain")
    raise HTTPException(status_code=404, detail="Log file not found")

@app.get("/processed")
async def get_processed():
    if os.path.exists(PROCESSED_FILE):
        with open(PROCESSED_FILE, "r") as f:
            return json.load(f)
    return []

@app.get("/status")
async def get_status():
    return {
        "current_week": current_processing["week"],
        "processed_weeks": current_processing["processed"],
        "errors_count": len(errors),
        "memory_info": await get_memory_info()
    }

if __name__ == "__main__":
    import uvicorn
    logger.info("üü¢ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–∞ http://0.0.0.0:8000")
    uvicorn.run(app, host="0.0.0.0", port=8000)